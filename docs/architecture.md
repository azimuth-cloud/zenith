# Zenith Architecture

This document describes the architecture of Zenith in detail.

Zenith has a client-server architecture in which the client and server collaborate to
establish a secure tunnel over which traffic is proxied in a controlled way from the
internet to a service that would not otherwise be exposed to the internet. This allows
services that are behind NAT or a firewall to be exposed to end-users while only
being bound locally. Those services can also (optionally) benefit from TLS termination
and authentication/authorization performed by Zenith at the proxy.

Zenith is mostly composed of industry-standard software and protocols such as
[OpenSSH](https://www.openssh.com/), [Hashicorp Consul](https://www.consul.io/) and the
[NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/), glued together
with a small amount of custom code and deployed using [Kubernetes](https://kubernetes.io/).

## Architecture Diagram

This diagram shows the components in the Zenith Architecture. Components are colour-coded
to show their ownership in four classes:

  * **Zenith Component**: The component is composed of custom Zenith code.
  * **Managed Component**: The component is managed as part of a Zenith installation.
  * **Kubernetes Component**: The component is part of or managed by the Kubernetes cluster.
  * **External Component**: The component is external to and not managed by Zenith.

Additionally, some optional components are also shown on the diagram.

![Zenith Architecture Diagram](./zenith-architecture.png?raw=true)

## Establishing a proxied service

The following describes the flow between the components that establish an instance of a
proxied service. Multiple instances of a proxied service can be bound to the same subdomain,
and traffic will be load-balanced by the Zenith server.

  1. The service to be proxied is running.
  1. The Zenith client is launched.
  1. The Zenith configuration wrapper reads the tunnel configuration and spawns an SSH client
     process that connects to the SSHD component of a Zenith server.
       * The SSH client requests a dynamically-allocated remote forwarded port from SSHD by using
         `0` as the remote port number with the `-R` option, i.e.
         `ssh -R 0:${service_host}:${service_port}`.
       * The `stdin/out/err` streams of the SSH client process are connected to pipes that the
         configuration wrapper controls.
  1. The SSH connection is assigned to an SSHD instance by the TCP load-balancer. This allocation
     persists for the duration of the SSH connection.
  1. SSHD responds to the SSH client with the allocated port on `stderr`, which is read by the
     client configuration wrapper.
  1. SSHD launches the `tunnel-init` script, whose `stdin/out/err` are connected to the
     SSH client process.
  1. The client configuration wrapper passes the tunnel configuration, including the
     allocated port, to the `tunnel-init` script on `stdin`. A subdomain can be specified
     by the client, or a random subdomain will be generated by the `tunnel-init` script.
  1. The `tunnel-init` script creates a service instance in Consul that associates the subdomain
     with the pod IP of the allocated SSHD instance and the allocated port for the tunnel.
       * There can be multiple instances associated with the same subdomain.
  1. The sync component of the Zenith server is notified of the change in Consul.
  1. The sync component creates or updates the `Endpoints`, `Service` and `Ingress` resources
     in Kubernetes to match the current service instances for the subdomain.
       * Each subdomain has one of each resource.
       * Each proxied service instance corresponds to a single entry in the `Endpoints`
         resource for the subdomain.
       * See [Services without selectors](https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors)
         for more information on how this works in Kubernetes.
  1. Traffic can now flow from the user to the proxied service via the Ingress Controller
     and SSH tunnel.

## Allocated port detection

Steps 5, 6 and 7, where the allocated port is returned by SSHD to the SSH client and then passed
back to the `tunnel-init` script via the configuration wrapper, are necessary because the allocated
port number is not made available to the spawned `tunnel-init` script by SSHD.

This obviously places a lot of trust in the client to report the allocated port correctly. It
would be preferable to detect the port from the `tunnel-init` script, but this appears to be very
difficult, and probably impossible without root.

Instead, we put in place some mitigations to prevent a malicious client from crafting a tunnel that
allows them to receive traffic that is not intended for them:

  * Clients are encouraged to use dynamically-allocated ports.
      * This is enforced when using the Zenith client.
      * This makes the allocated port for a proxied service harder to guess, and so it is harder
        for a malicious client to connect a known domain in order to access the proxied service.
  * Clients are encouraged to use subdomains that are hard-to-guess.
      * This makes it harder for a malicious client to discover a valid domain and bind their
        service to it in order to intercept traffic.
  * Only allow a subdomain to be bound to a port that is listening.
      * This prevents a malicious client from pre-binding a known domain to a port that is not
        yet in use in the hope of intercepting traffic in the future.
  * Only allow one subdomain to be bound to each tunnel.
      * This prevents a malicious client from binding an additional subdomain that is known
        to them to an existing tunnel in order to access that service.
  * SSHD is configured so that the `tunnel-init` script is the only command that can be run.
      * This prevents a malicious client from running another command to collect information
        about the bound ports or to contact Consul.
  * SSHD is configured so that it only permits remote port forwarding, not local or dynamic
    port forwarding (see
    [SSH port forwarding](https://help.ubuntu.com/community/SSH/OpenSSH/PortForwarding)).
      * This prevents a malicious client from setting up a local port-forward to the
        bound port for another service and sending traffic directly to it, bypassing the
        Zenith proxying and any associated authentication.

## Why not use Consul Service Sync?

Consul does have a [Service Sync component](https://www.consul.io/docs/k8s/service-sync) that
will synchronise Consul services with Kubernetes services, however this is implemented using
[ExternalName services](https://kubernetes.io/docs/concepts/services-networking/service/#externalname)
rather than `Endpoints`.

This is not suitable for Zenith because the port numbers that are assigned for the remote
forwarded ports are not predictable. Also, in the case where multiple instances of a proxied
service are registered for the same subdomain they will be allocated different port numbers.
Using `Endpoints` rather than `ExternalName` services allows this to work.

## SSHD hardening

Zenith relies heavily on the SSH protocol to establish secure tunnels. However we want to
prevent the abuse of the power of SSH by malicious clients. To do this, SSHD is hardened
in several ways:

  * Disabling all unnecessary features, e.g. agent forwarding, X11 forwarding, local and
    dynamic port forwarding.
  * Using `ForceCommand` to force the `tunnel-init` script to run on every connection.
  * Using `AllowUsers` to permit connections only for a single, non-root user (called `zenith`).
  * Running SSHD as the `zenith` user (so that changing to another user is not possible).
  * Setting the login shell of the `zenith` user to [rbash](https://en.wikipedia.org/wiki/Restricted_shell),
    a restricted shell, so that the operations that can be performed are restricted even
    if `ForceCommand` is bypassed (which shouldn't happen anyway!).
  * Only forwarding whitelisted environment variables into spawned connections that are known
    to be part of the Zenith configuration.
  * Running the SSHD containers with a tight
    [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
    that:
      * Enforces a read-only root file-system, so that unexpected configuration changes cannot occur.
      * Enforces that the container runs as a non-root user.
      * Drops all
        [Linux capabilities](https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/)
        from the container.
